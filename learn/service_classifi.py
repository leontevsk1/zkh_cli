# servclassification.py
import sys
from pathlib import Path
sys.path.append(str(Path(__file__).resolve().parents[1]))
import re, json
import numpy as np
import pandas as pd
from typing import Dict, List
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report, f1_score, confusion_matrix
from scipy.sparse import hstack
from sklearn.pipeline import Pipeline, FeatureUnion
import joblib

from src.ml.utils.rule_features import RuleFeatures  # <-- üí° –æ—Ç–¥–µ–ª—å–Ω–æ, —É–∂–µ –µ—Å—Ç—å

# === –ü—É—Ç–∏ ===
PROJECT_ROOT = Path(__file__).resolve().parents[1]
DATA_CSV = PROJECT_ROOT / "learn/content/L_corrected.csv"
OUT_DIR = PROJECT_ROOT / "src/ml/models/service_clf"
OUT_DIR.mkdir(parents=True, exist_ok=True)

TEXT_COL = "Desc"
LABEL_COL = "Group"
MAJOR_CLASS = "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–º–æ–º"

# === –õ–µ–∫—Å–∏–∫–æ–Ω—ã ===
LEX: Dict[str, List[str]] = {
    "–ö–∞–Ω–∞–ª–∏–∑–∞—Ü–∏—è": [r"\b–∫–∞–Ω–∞–ª–∏–∑–∞—Ü", r"\b–∑–∞—Å–æ—Ä", r"\b–ø—Ä–æ—á–∏—Å—Ç–∫", r"\b–∫–æ–ª–æ–¥–µ—Ü", r"\b—Å—Ç–æ—è–∫", r"\b—Å–∏—Ñ–æ–Ω", r"\b—É–Ω–∏—Ç–∞–∑", r"\b—Ä–∞–∫–æ–≤–∏–Ω", r"\b—Å–ª–∏–≤", r"\b—Ñ–∞–Ω–æ–≤–∞", r"\b–∑–∞–ø–∞—Ö(?!.*–≥–∞–∑–∞)", r"\b–∂–∏—Ä–æ—É–ª–∞–≤", r"\b–∫—Ä—ã—à–∫–∞ –∫–æ–ª–æ–¥—Ü–∞"],
    "–í–æ–¥–æ–æ—Ç–≤–µ–¥–µ–Ω–∏–µ": [r"\b–≤–æ–¥–æ–æ—Ç–≤–µ–¥", r"\b–∫–Ω—Å\b", r"\b–ª–∏–≤–Ω–µ–≤", r"\b–¥–æ–∂–¥–µ–ø—Ä–∏", r"\b—Å—Ç–æ–∫–∏", r"\b—Ñ–µ–∫–∞–ª—å", r"\b–∑–∞—Ç–æ–ø(–∏–ª[–∞–∏]|–∏–ª–æ)\b", r"\b–ø–æ–¥–≤–∞–ª–µ\b.*\b–≤–æ–¥–∞\b", r"\b–ø–µ—Ä–µ–ª–∏–≤"],
    "–≠–ª–µ–∫—Ç—Ä–æ—ç–Ω–µ—Ä–≥–∏—è": [r"\b–Ω–µ—Ç\s+—Å–≤–µ—Ç–∞", r"\b–≤—ã–±–∏–ª[–∞–∏].*\b–ø—Ä–æ–±–∫", r"\b–ø—Ä–æ–±–∫[–∞–∏]\b", r"\b–∞–≤—Ç–æ–º–∞—Ç(—ã)?\b", r"\b—â–∏—Ç–æ–∫", r"\b–∑–∞–º—ã–∫–∞–Ω", r"\b–∫–æ—Ä–æ—Ç–∏—Ç", r"\b–∏—Å–∫—Ä–∏—Ç", r"\b–ø—Ä–æ–≤–æ–¥", r"\b—Ä–æ–∑–µ—Ç–∫", r"\b–æ—Å–≤–µ—â–µ–Ω", r"\b–ª–∞–º–ø(–∞|—ã|–æ—á–µ–∫|–æ—á–∫—É)"],
    "–ì–í–°": [r"\b–≥–≤—Å\b", r"\b–≥–æ—Ä—è—á(–∞—è|–µ–π)\s+–≤–æ–¥", r"\b–Ω–µ—Ç\s+–≥–æ—Ä—è—á", r"\b—Ç–µ–º–ø–µ—Ä–∞—Ç—É—Ä", r"\b—Å—á–µ—Ç—á–∏–∫", r"\b–æ–ø–ª–æ–º–±", r"\b–ø–ª–æ–º–±", r"\b–ø–æ–¥–º–µ—Å", r"\b—Ü–∏—Ä–∫—É–ª—è—Ü"],
    "–õ–∏—Ñ—Ç—ã": [r"\b–ª–∏—Ñ—Ç", r"\b–∑–∞—Å—Ç—Ä—è–ª", r"\b–∑–∞—Å—Ç—Ä—è–ª–∏", r"\b–Ω–µ\s+—Ä–∞–±–æ—Ç–∞–µ—Ç\s+–ª–∏—Ñ—Ç", r"\b–¥–≤–µ—Ä(—å|–∏)\s+(–Ω–µ\s+)?(–∑–∞–∫—Ä—ã–≤–∞|–æ—Ç–∫—Ä—ã–≤–∞)", r"\b–∫–Ω–æ–ø–∫[–∞–∏]\s+–≤—ã–∑–æ–≤–∞", r"\b–∫–∞–±–∏–Ω[–∞–µ]", r"\b—Å–∫—Ä–µ–∂–µ—Ç", r"\b—Ä–µ–≤–µ—Ä—Å"],
    "–î–æ–º–æ—Ñ–æ–Ω—ã": [r"\b–¥–æ–º–æ—Ñ–æ–Ω", r"\b—Ç—Ä—É–±–∫", r"\b–ø–∞–Ω–µ–ª", r"\b–≤—ã–∑–æ–≤", r"\b–Ω–µ\s+—Ä–∞–±–æ—Ç–∞–µ—Ç\s+–¥–æ–º–æ—Ñ–æ–Ω", r"\b–∫–ª—é—á(–∏|–µ–π)\b", r"\b–º–∞–≥–Ω–∏—Ç", r"\b–¥–≤–µ—Ä(—å|–∏)\b.*\b–Ω–µ\s+–æ—Ç–∫—Ä—ã–≤–∞", r"\b–∫–æ–¥(—ã)?", r"\b–∑–≤–æ–Ω–æ–∫"],
    "–£–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –¥–æ–º–æ–º": [r"\b–ø–æ–¥—ä–µ–∑–¥", r"\b–¥–≤–æ—Ä", r"\b—É–±–æ—Ä–∫[–∞–∏]", r"\b–º—É—Å–æ—Ä", r"\b—Å–Ω–µ–≥", r"\b–Ω–∞–ª–µ–¥—å|\b–≥–æ–ª–æ–ª–µ–¥", r"\b–∫—Ä–æ–≤–ª|–∫—Ä—ã—à–∞", r"\b–¥–≤–µ—Ä(—å|–∏)\s+–ø–æ–¥—ä–µ–∑–¥–∞", r"\b–ø–µ—Ä–∏–ª–∞", r"\b–ø–æ—á–∏—Å—Ç–∏—Ç—å"],
    "–û—Ç–æ–ø–ª–µ–Ω–∏–µ": [r"\b–Ω–µ—Ç\s+–æ—Ç–æ–ø–ª–µ–Ω", r"\b–±–∞—Ç–∞—Ä–µ(—è|–∏)\s+(—Ö–æ–ª–æ–¥|–ª–µ–¥)", r"\b—Ä–∞–¥–∏–∞—Ç–æ—Ä", r"\b—Å—Ç–æ—è–∫\s+–æ—Ç–æ–ø–ª–µ–Ω", r"\b–∫–æ—Ç–µ–ª"],
    "–ì–∞–∑–æ—Å–Ω–∞–±–∂–µ–Ω–∏–µ": [r"\b–≥–∞–∑\b", r"\b–∑–∞–ø–∞—Ö\s+–≥–∞–∑–∞", r"\b—É—Ç–µ—á–∫[–∞–∏]\s+–≥–∞–∑–∞", r"\b–≥–∞–∑–æ–≤", r"\b–ø–ª–æ–º–±.*–≥–∞–∑"]
}

def normalize_text(s: str) -> str:
    return re.sub(r"\s+", " ", s.lower()).strip()

# === –î–∞–Ω–Ω—ã–µ ===
df = pd.read_csv(DATA_CSV)
df = df[[TEXT_COL, LABEL_COL]].dropna()
df[TEXT_COL] = df[TEXT_COL].astype(str).map(normalize_text)

X_train, X_val, y_train, y_val = train_test_split(
    df[TEXT_COL].values, df[LABEL_COL].values, test_size=0.2, stratify=df[LABEL_COL], random_state=42
)

# === –§–∏—á–∏ ===
classes_sorted = sorted(pd.unique(df[LABEL_COL]))
tfidf = TfidfVectorizer(ngram_range=(1, 2), min_df=3, max_df=0.9)

# === –û–±—ä–µ–¥–∏–Ω—è–µ–º –ø—Ä–∏–∑–Ω–∞–∫–∏ ===
features = FeatureUnion([
    ("tfidf", tfidf),
    ("rules", RuleFeatures(class_labels=classes_sorted))  # <-- rule-based –ø—Ä–∏–∑–Ω–∞–∫–∏
])

# === –ö–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä ===
clf = LogisticRegression(max_iter=2000, class_weight="balanced", n_jobs=-1)

# === Pipeline ===
pipeline = Pipeline([
    ("features", features),
    ("clf", clf)
])

# === –û–±—É—á–µ–Ω–∏–µ ===
sample_weight = np.ones(len(y_train), dtype=np.float32)
sample_weight[np.array(y_train) == MAJOR_CLASS] = 0.6
pipeline.fit(X_train, y_train, clf__sample_weight=sample_weight)

# === –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –∏ –æ—Ç—á—ë—Ç ===
y_pred = pipeline.predict(X_val)
print("\n=== –ë–µ–∑ –ø–æ—Ä–æ–≥–∞ ===")
print(classification_report(y_val, y_pred, digits=3))
print("Confusion matrix:\n", confusion_matrix(y_val, y_pred, labels=clf.classes_))

# === –≠–∫—Å–ø–æ—Ä—Ç –º–æ–¥–µ–ª–∏ ===
joblib.dump(pipeline, OUT_DIR / "service_clf.joblib")

# === –≠–∫—Å–ø–æ—Ä—Ç –º–µ—Ç–∞–¥–∞–Ω–Ω—ã—Ö ===
meta = {
    "version": "1.1",
    "source": "servclassification.py",
    "feature_union": ["TfidfVectorizer", "RuleFeatures"],
    "num_classes": len(classes_sorted)
}
(OUT_DIR / "service_meta.json").write_text(json.dumps(meta, ensure_ascii=False, indent=2), encoding="utf-8")

print(f"‚úÖ –≠–∫—Å–ø–æ—Ä—Ç –∑–∞–≤–µ—Ä—à—ë–Ω: {OUT_DIR}")